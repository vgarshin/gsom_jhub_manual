{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "simple-pilot",
   "metadata": {},
   "source": [
    "# Scraping and Parsing Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-giant",
   "metadata": {},
   "source": [
    "![Harry Potter Parsing](images/parser_tongue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-rubber",
   "metadata": {},
   "source": [
    "Many data analysis projects require gathering and processind data from the Internet site pages. Following code example will help you with basic tools of scraping and parsind data from the sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-wisdom",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pymorphy2\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import tokenizers\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "MORPH = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-directive",
   "metadata": {},
   "source": [
    "## Get text from MiBA's page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-quantity",
   "metadata": {},
   "source": [
    "Let's get the text from [Master in Business Analytics and Big Data (MiBA)](https://gsom.spbu.ru/en/programmes/graduate/miba/) internet site page. We use standard `urllib` library to get `html` data from the page and [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) library to parce `html`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_2_SCRAP = 'https://gsom.spbu.ru/en/programmes/graduate/miba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(URL_2_SCRAP)\n",
    "response = urlopen(request)\n",
    "html = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "    page_text = soup.get_text()\n",
    "    for ch in ['\\n', '\\t', '\\r']:\n",
    "        page_text = page_text.replace(ch, ' ')\n",
    "    return ' '.join(page_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_page = get_text(html)\n",
    "print('sample of text:', text_from_page[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-barcelona",
   "metadata": {},
   "source": [
    "Save parsed text to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/miba_page.txt', 'w') as file:\n",
    "    file.write(text_from_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-investor",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-railway",
   "metadata": {},
   "source": [
    "Read text from a file and do basic preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/miba_page.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    for ch in ['\\n', '\\t', '\\r']:\n",
    "        text = text.replace(ch, ' ')\n",
    "    result = re.sub('[^а-яА-Яa-zA-Z]+', ' ', text).strip().lower()\n",
    "    result = re.sub('ё', 'е', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessing(text)\n",
    "print('total symbols:', len(text))\n",
    "print('sample of text:', text[2200:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-vessel",
   "metadata": {},
   "source": [
    "## More processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-attendance",
   "metadata": {},
   "source": [
    "Get words in the text to the [dictionary form](https://en.wikipedia.org/wiki/Lemmatisation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advprocessing(text):\n",
    "    funсtion_words = {'INTJ', 'PRCL', 'CONJ', 'PREP'}\n",
    "    lemmatized_words = list(map(lambda word: MORPH.parse(word)[0], text.split()))\n",
    "    result = []\n",
    "    for word in lemmatized_words:\n",
    "        if word.tag.POS not in funсtion_words:\n",
    "            result.append(word.normal_form)\n",
    "    return result, ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens, text = advprocessing(text)\n",
    "print('total symbols:', len(text))\n",
    "print('total words:', len(text_tokens))\n",
    "print('sample of text:', text[2200:2500])\n",
    "print('sample of text tokens:', text_tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-wisdom",
   "metadata": {},
   "source": [
    "## Some visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-gilbert",
   "metadata": {},
   "source": [
    "Prepare and display some diagrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(text_tokens)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('most common 10 words:', freq_dist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('50 more frequent words in text')\n",
    "freq_dist.plot(50, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white').generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-liquid",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
