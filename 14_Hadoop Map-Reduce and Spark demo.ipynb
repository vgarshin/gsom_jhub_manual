{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop, Map-Reduce and Spark demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import utils as pu\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = '/jovyan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_dirs(path, filter_str=''):\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-ls', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    dirs = out.decode('utf-8').split('\\n')\n",
    "    dirs = list(filter(lambda x: filter_str in x, dirs))\n",
    "    dirs = list(map(lambda x: x.split(' ')[-1], dirs))\n",
    "    return dirs\n",
    "\n",
    "def file_content(path):\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-cat', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    return out.decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put ./data/telecom_churn.csv {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_dirs(WORK_DIR, 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = file_content(f'{WORK_DIR}/telecom_churn.csv')\n",
    "print(content[:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '4g')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\n",
    "    f'{WORK_DIR}/telecom_churn.csv',\n",
    "    sep=',', \n",
    "    header=True\n",
    ")\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rows in spark dataframe:', sdf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Test Hadoop cluster by running wordcount task\n",
    "# based on https://github.com/Segence/docker-hadoop\n",
    "# by Rob Vadai https://twitter.com/robvadai\n",
    "\n",
    "work_dir=/jovyan\n",
    "temp_dir=tohdfs\n",
    "\n",
    "# create input files\n",
    "mkdir -p ${temp_dir}\n",
    "echo \"Er legt die Nadel auf die Ader Und bittet die Musik herein Zwischen Hals und Unterarm Die Melodie fÃ¤hrt leise ins Gebein\" > ${temp_dir}/part1.txt\n",
    "echo \"Er hat die Augen zugemacht In seinem Blut tobt eine Schlacht Ein Heer marschiert durch seinen Darm Die Eingeweide werden langsam warm\" > ${temp_dir}/part2.txt\n",
    "\n",
    "# create input directory on HDFS\n",
    "hadoop fs -mkdir -p ${work_dir}/input\n",
    "\n",
    "# put input files to HDFS\n",
    "hdfs dfs -put ./${temp_dir}/* ${work_dir}/input\n",
    "\n",
    "# run wordcount\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar wordcount ${work_dir}/input ${work_dir}/output\n",
    "\n",
    "# print the input files\n",
    "echo -e \"\\ninput file1.txt:\"\n",
    "hdfs dfs -cat ${work_dir}/input/part1.txt\n",
    "\n",
    "echo -e \"\\ninput file2.txt:\"\n",
    "hdfs dfs -cat ${work_dir}/input/part2.txt\n",
    "\n",
    "# print the output of wordcount\n",
    "echo -e \"\\nwordcount output:\"\n",
    "hdfs dfs -cat ${work_dir}/output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
