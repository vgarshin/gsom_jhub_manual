{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smooth-catholic",
   "metadata": {},
   "source": [
    "# Hadoop, Map-Reduce and Spark demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-technician",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-aquarium",
   "metadata": {},
   "source": [
    "__NOTE:__ for this notebook you should start your server with `Hadoop (with YARN) and Spark environment`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-netherlands",
   "metadata": {},
   "source": [
    "![Hadoop in a box](images/hadoop_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-victory",
   "metadata": {},
   "source": [
    "[The Apache Hadoop software library](https://hadoop.apache.org/) is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-closing",
   "metadata": {},
   "source": [
    "The current installation includes following modules from the Hadoop ecosystem:\n",
    "\n",
    "- __Hadoop Common:__ The common utilities that support the other Hadoop modules.\n",
    "- __Hadoop Distributed File System (HDFS™):__ A distributed file system that provides high-throughput access to application data.\n",
    "- __Hadoop YARN:__ A framework for job scheduling and cluster resource management.\n",
    "- __Hadoop MapReduce:__ A YARN-based system for parallel processing of large data sets.\n",
    "- __Spark™:__ A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-civilization",
   "metadata": {},
   "source": [
    "Pseudo Distributed Mode (Single Node Cluster)\n",
    "\n",
    "Hadoop can also be run on a single-node in a [pseudo-distributed mode](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation) where each Hadoop daemon runs in a separate Java process.\n",
    "\n",
    "In Pseudo-distributed Mode we also use only a single node, but the main thing is that the cluster is simulated, which means that all the processes inside the cluster will run independently to each other. All the daemons that are Namenode, Datanode, Secondary Name node, Resource Manager, Node Manager, etc. will be running as a separate process on separate JVM(Java Virtual Machine) or we can say run on different java processes that is why it is called a Pseudo-distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-healing",
   "metadata": {},
   "source": [
    "<font color='red'>__VERY IMPORTANT NOTE:__ The Hadoop instance installed within 'Hadoop (with YARN) and Spark environment' was designed only for educational purposes and DOES NOT STORE DATA after you stop your server. You can create or delete files in HDFS filesystem, write data during session, but next time you start Jupyter server there will be clear filesystem with no data in it.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-usage",
   "metadata": {},
   "source": [
    "## Libraries for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import utils as pu\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "YARN_PORT = 8088\n",
    "\n",
    "# working directory for default user `jovyan`\n",
    "WORK_DIR = '/jovyan'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-heaven",
   "metadata": {},
   "source": [
    "## HDFS operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-ordering",
   "metadata": {},
   "source": [
    "[The Hadoop Distributed File System (HDFS)](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-things",
   "metadata": {},
   "source": [
    "Navigation through HDFS is available with `hdfs dfs` [commands](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html) which are quite simular to Unix shell navigation (`ls`, `cat`, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_dirs(path, filter_str=''):\n",
    "    \"\"\"\n",
    "    Returns files in path provided as a list. \n",
    "    File names may be filtered by `filter_str` parameter,\n",
    "    e.g. `filter_str='csv'` will display only `csv` files.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-ls', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    dirs = out.decode('utf-8').split('\\n')\n",
    "    dirs = list(filter(lambda x: filter_str in x, dirs))\n",
    "    dirs = list(map(lambda x: x.split(' ')[-1], dirs))\n",
    "    return dirs\n",
    "\n",
    "def file_content(path):\n",
    "    \"\"\"\n",
    "    Returns content of the file.\n",
    "    Similar to `cat` command.\n",
    "    \n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(\n",
    "        ['hdfs', 'dfs', '-cat', path], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    out, err = process.communicate()\n",
    "    return out.decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list root directory\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list working directory '/jovyan'\n",
    "# NOTE: variable WORK_DIR='/jovyan' used in braces\n",
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-university",
   "metadata": {},
   "source": [
    "You may put files in HDFS with `-put` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put local file 'telecom_churn.csv' in\n",
    "!hdfs dfs -put ./data/telecom_churn.csv {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if file appeared in HDFS\n",
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function defined above\n",
    "hdfs_dirs(WORK_DIR, 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the content of the 'telecom_churn.csv' file\n",
    "content = file_content(f'{WORK_DIR}/telecom_churn.csv')\n",
    "print(content[:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-racing",
   "metadata": {},
   "source": [
    "## Map-reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-russian",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-restriction",
   "metadata": {},
   "source": [
    "[Hadoop MapReduce is a software framework](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.\n",
    "\n",
    "A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-charles",
   "metadata": {},
   "source": [
    "The MapReduce framework operates exclusively on `<key, value>` pairs, that is, the framework views the input to the job as a set of `<key, value>` pairs and produces a set of `<key, value>` pairs as the output of the job, conceivably of different types.\n",
    "\n",
    "Input and Output types of a MapReduce job:\n",
    "```\n",
    "(input) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-monkey",
   "metadata": {},
   "source": [
    "Demo for MapReduce framework is for well known task of word count:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-hydrogen",
   "metadata": {},
   "source": [
    "![MapReduce](images/mapreducescheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-cannon",
   "metadata": {},
   "source": [
    "### WordCount with Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-florida",
   "metadata": {},
   "source": [
    "`WordCount` is a simple application that counts the number of occurrences of each word in a given input set. For this demo ready `jar` package is used.\n",
    "\n",
    "First let's copy files to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# create input directory on HDFS\n",
    "hdfs dfs -mkdir -p ${work_dir}/input\n",
    "\n",
    "# put files to HDFS\n",
    "hdfs dfs -put ./data/wordcount/verse* ${work_dir}/input\n",
    "hdfs dfs -ls ${work_dir}/input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-triumph",
   "metadata": {},
   "source": [
    "Run a map-reduce job and enjoy long logs output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# delete directory if exists\n",
    "#hdfs dfs -rm -r ${work_dir}/output\n",
    "\n",
    "# run wordcount\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar wordcount \\\n",
    "    ${work_dir}/input ${work_dir}/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls {WORK_DIR}/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "\n",
    "# print the input files\n",
    "echo -e \"\\ninput 1st file1:\"\n",
    "hdfs dfs -cat ${work_dir}/input/verse1.txt\n",
    "echo -e \"\\ninput 2nd file:\"\n",
    "hdfs dfs -cat ${work_dir}/input/verse2.txt\n",
    "\n",
    "# print the output of wordcount\n",
    "echo -e \"\\nwordcount output:\"\n",
    "hdfs dfs -cat ${work_dir}/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-margin",
   "metadata": {},
   "source": [
    "### WordCount with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-still",
   "metadata": {},
   "source": [
    "Next example will use [Hadoop streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) concept. Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer. \n",
    "\n",
    "In the example nelow, both the mapper and the reducer are executables that read the input from stdin (line by line) and emit the output to stdout. The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-secretariat",
   "metadata": {},
   "source": [
    "Put a file to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put ./data/wordcount/lizard_king.txt {WORK_DIR}\n",
    "!hdfs dfs -ls {WORK_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-state",
   "metadata": {},
   "source": [
    "Two Python scripts are used `mapper.py` and `reducer.py`, let's look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"\\n************** MAPPER.PY ****************\\n\"\n",
    "cat ./manutils/mapreduce/mapper.py\n",
    "echo -e \"\\n************** REDUCER.PY ****************\\n\"\n",
    "cat ./manutils/mapreduce/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-david",
   "metadata": {},
   "source": [
    "Run the job and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "work_dir=/jovyan\n",
    "out_dir=/lizard_king_output\n",
    "\n",
    "# delete directory if exists\n",
    "#hdfs dfs -rm -r ${work_dir}${out_dir}\n",
    "\n",
    "yarn jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \\\n",
    "    -input ${work_dir}/lizard_king.txt -output ${work_dir}${out_dir} \\\n",
    "    -file ./manutils/mapreduce/mapper.py -file ./manutils/mapreduce/reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" -reducer \"python3 reducer.py\"\n",
    "\n",
    "hdfs dfs -ls ${work_dir}/${out_dir}\n",
    "hdfs dfs -cat ${work_dir}/${out_dir}/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-startup",
   "metadata": {},
   "source": [
    "### YARN jobs monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-laptop",
   "metadata": {},
   "source": [
    "Hadoop also provided YARN Web UI for Yarn Resource manager. All the jobs (submitted, running or finished) can be traced in YARN Web UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'YARN Web UI available at:',\n",
    "    'https://jhas01.gsom.spbu.ru{}proxy/{}/cluster'.format(\n",
    "        os.environ['JUPYTERHUB_SERVICE_PREFIX'],\n",
    "        YARN_PORT\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-synthesis",
   "metadata": {},
   "source": [
    "## Spark with Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-values",
   "metadata": {},
   "source": [
    "[Apache Spark](https://spark.apache.org) is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "\n",
    "There are three options to deploy Spark with Hadoop:\n",
    "- [__Spark with Mesos__](https://spark.apache.org/docs/latest/running-on-mesos.html): when using Mesos, the Mesos master replaces the Spark master as the cluster manager\n",
    "- [__Spark with YARN__](https://spark.apache.org/docs/latest/running-on-yarn.html): Spark jobs are managed with YARN\n",
    "- [__Standalone Spark__](https://spark.apache.org/docs/latest/spark-standalone.html): no YARN or Mesos job managers\n",
    "\n",
    "We use standalone Spark installation to process data in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '4g')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-technique",
   "metadata": {},
   "source": [
    "Read file from HDFS as Spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\n",
    "    f'{WORK_DIR}/telecom_churn.csv',\n",
    "    sep=',', \n",
    "    header=True\n",
    ")\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rows in spark dataframe:', sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-review",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
