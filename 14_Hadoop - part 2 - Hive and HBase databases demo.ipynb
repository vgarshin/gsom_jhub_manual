{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smooth-catholic",
   "metadata": {},
   "source": [
    "# Hadoop: Hive and HBase databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-technician",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-aquarium",
   "metadata": {},
   "source": [
    "__NOTE:__ for this notebook you should start your server with `Hadoop (with YARN) and Spark environment`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-netherlands",
   "metadata": {},
   "source": [
    "![Hadoop in a box](images/hadoop_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-victory",
   "metadata": {},
   "source": [
    "[The Apache Hadoop software library](https://hadoop.apache.org/) is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-healing",
   "metadata": {},
   "source": [
    "<font color='red'>__VERY IMPORTANT NOTE:__ The Hadoop instance installed within 'Hadoop (with YARN) and Spark environment' was designed only for educational purposes and DOES NOT STORE DATA after you stop your server. You can create or delete files in HDFS filesystem, write data during session, but next time you start Jupyter server there will be clear filesystem with no data in it.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81083a-3c56-412a-9f4a-886fb1573714",
   "metadata": {},
   "source": [
    "## Hive database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1aa4ac-c7d9-4d46-a5b2-f6974118fe1d",
   "metadata": {},
   "source": [
    "[The Apache Hive](https://hive.apache.org/) is a data warehouse software project __built on top of Apache Hadoop__ for providing data query and analysis. It is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage __using SQL__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec22e4-c5dc-442c-af26-0c79657924c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290cb7c9-b2b8-49b0-8251-a9b8a9ba4bf5",
   "metadata": {},
   "source": [
    "Source of data is Kaggle [dataset on flights](https://www.kaggle.com/datasets/goyaladi/flight-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2e505-ba24-4ee7-b2d6-4b6ca4fbbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy source file to HDFS\n",
    "!hdfs dfs -put ./data/flight_data.csv /jovyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabeafa-c23d-466d-bc26-e8a99172e887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list files in HDFS\n",
    "!hdfs dfs -ls /jovyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3321b-afc7-4365-bb14-5dc8a10b16b0",
   "metadata": {},
   "source": [
    "Will use file with the data for video games sales in different countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304deb63-be32-4ac6-835e-118c2dfba64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -head /jovyan/flight_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6acc83-0dda-49ed-b85b-c9c406294ef7",
   "metadata": {},
   "source": [
    "### Create table in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbef44-c423-4d0c-95ae-c845a67c251b",
   "metadata": {},
   "source": [
    "We need to create a table in Hive to put the data in that table on the next stage. You may want to read about [Hive data types](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bc12c-4570-4bee-a41e-bea659e02772",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hive -e \\\n",
    "    \"CREATE TABLE flights ( \\\n",
    "        DepartureCity STRING, \\\n",
    "        ArrivalCity STRING, \\\n",
    "        DepartureDate TIMESTAMP, \\\n",
    "        FlightDuration FLOAT, \\\n",
    "        DelayMinutes INT, \\\n",
    "        CustomerID INT, \\\n",
    "        Name STRING, \\\n",
    "        BookingClass STRING, \\\n",
    "        FrequentFlyerStatus STRING, \\\n",
    "        Route STRING, \\\n",
    "        TicketPrice FLOAT, \\\n",
    "        CompetitorPrice FLOAT, \\\n",
    "        Demand FLOAT, \\\n",
    "        Origin STRING, \\\n",
    "        Destination STRING, \\\n",
    "        Profitability FLOAT, \\\n",
    "        LoyaltyPoints INT, \\\n",
    "        Churned BOOLEAN) \\\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \\\n",
    "    LINES TERMINATED BY '\\n' \\\n",
    "    TBLPROPERTIES('skip.header.line.count'='1');\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730b1de-db07-4ac6-a1f5-d433b6c24f03",
   "metadata": {},
   "source": [
    "Hive produces a lot of logs, so it is recommended to create a file to write output in this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bb733-7826-4521-b56c-9cff66f3ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch result.txt\n",
    "!echo ---------------------------- >> result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589b8ff-2307-461b-80e9-9df461d5e08b",
   "metadata": {},
   "source": [
    "Let's put data into table created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9ab19-378f-4b9a-a87a-780dcf3897d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hive -e \"LOAD DATA INPATH '/jovyan/flight_data.csv' OVERWRITE INTO TABLE flights\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7446337-ea13-459b-b405-7076506e130e",
   "metadata": {},
   "source": [
    "__NOTE__ that after loading the data, the source file will be deleted from the source location, and the file loaded to the Hive data warehouse location or to the LOCATION specified while creating a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240f1b6-901d-4077-a850-fbbcc931ce3e",
   "metadata": {},
   "source": [
    "### SQL queries to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d5cbe-2803-4555-aae4-ee94576961f0",
   "metadata": {},
   "source": [
    "Test query to a new table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e796b5-fa77-4188-a69c-ece52e93adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hive -S -e \"SELECT * FROM flights LIMIT 5\" >> result.txt\n",
    "!echo ---------------------------- >> result.txt\n",
    "!cat result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e37dc-1037-4807-949b-9ee4d3212bc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "More complicated query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a3436-0e7d-4bef-ab54-f2cece14c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hive -S -e \"SELECT FrequentFlyerStatus, avg(DelayMinutes) FROM flights GROUP BY FrequentFlyerStatus\" >> result.txt\n",
    "!echo ---------------------------- >> result.txt\n",
    "!cat result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ad80c-4dbf-450e-bdfc-b78a6540a53a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HBase database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ca33c-4628-4bb9-82d3-93124d2a2e1a",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59335345-046d-477c-a653-37fa8f18d194",
   "metadata": {},
   "source": [
    "[Apache HBase](https://hbase.apache.org/) is an open-source non-relational distributed database. You may try it with a very basic example [based on manual](https://hbase.apache.org/book.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542bc79-392d-4fd7-8b7c-21effe77f3a2",
   "metadata": {},
   "source": [
    "To run HBase within  `Hadoop (with YARN) and Spark environment` you may need to open a terminal and type `hbase shell`. Then you may try following commands in HBase shell:\n",
    "```\n",
    "hbase> create 'test', 'cf'\n",
    "hbase> list 'test'\n",
    "hbase> describe 'test'\n",
    "hbase> put 'test', 'row1', 'cf:a', 'value1'\n",
    "hbase> scan 'test'\n",
    "hbase> get 'test', 'row1'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7802e5-b6b0-4857-a0d8-74afcfab2220",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e27980-52dd-441b-9b46-9f251e576312",
   "metadata": {},
   "source": [
    "We can work with data, but [some libraries are needed](https://sparkbyexamples.com/hbase/hbase-table-filtering-data-like-where-clause/).\n",
    "```\n",
    "hbase> import org.apache.hadoop.hbase.filter.SingleColumnValueFilter \n",
    "hbase> import org.apache.hadoop.hbase.filter.CompareFilter\n",
    "hbase> import org.apache.hadoop.hbase.filter.BinaryComparator\n",
    "hbase> scan 'test', { FILTER => SingleColumnValueFilter.new(Bytes.toBytes('cf'), Bytes.toBytes('a'), CompareFilter::CompareOp.valueOf('EQUAL'),BinaryComparator.new(Bytes.toBytes('value1')))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c437d-9a23-43d8-bee7-f6b4f1901040",
   "metadata": {},
   "source": [
    "Create table in HBase shell:\n",
    "```\n",
    "hbase> create 'geo', 'city', 'provcode', 'provname', 'regcode', 'regname', 'postcode', 'salekey', 'ip'\n",
    "hbase> list 'geo'\n",
    "hbase> describe 'geo'\n",
    "hbase> scan 'geo'\n",
    "```\n",
    "\n",
    "Import data from file with help of terminal:\n",
    "```\n",
    "$ hbase org.apache.hadoop.hbase.mapreduce.ImportTsv \\\n",
    "    -Dimporttsv.separator=',' \\\n",
    "    -Dimporttsv.columns=HBASE_ROW_KEY,city,provcode,provname,regcode,regname,postcode,salekey,ip \\\n",
    "    geo ~/__MANUAL/data/dim_geo.csv\n",
    "```\n",
    "\n",
    "Search in database:\n",
    "```\n",
    "hbase> get 'geo', '9'\n",
    "hbase> import org.apache.hadoop.hbase.filter.SingleColumnValueFilter \n",
    "hbase> import org.apache.hadoop.hbase.filter.CompareFilter\n",
    "hbase> import org.apache.hadoop.hbase.filter.BinaryComparator\n",
    "hbase> scan 'geo', { FILTER => SingleColumnValueFilter.new(Bytes.toBytes('city'), Bytes.toBytes(''), CompareFilter::CompareOp.valueOf('EQUAL'),BinaryComparator.new(Bytes.toBytes('Weston')))}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af28656-e488-401b-b26b-6fda24a3f9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
