{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2589c87-362c-4779-b809-1d7a7793855e",
   "metadata": {},
   "source": [
    "# Использование LLM моделей в режиме чат-бота"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4ae5a-e572-456d-882a-3fec76a4fca8",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695bd5a2-52c6-4aec-896d-23357d1a220b",
   "metadata": {},
   "source": [
    "__NOTE:__ for this notebook you should start your server with `Data Science environment with LLL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0daa1e-db66-4639-bbc8-add96e0bcbea",
   "metadata": {},
   "source": [
    "![Apache Airflow in a box](images/jupyterlab_llm_env.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a829b-b6a5-4570-91a4-8017ce9086f5",
   "metadata": {},
   "source": [
    "## 1. Доступные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4664565-27d9-4e83-b64c-54cf7c7fb4fd",
   "metadata": {},
   "source": [
    "Для скачивания доступны модели, перечисленные в таблице ниже. Рекомендуется выбирать для использования модели размером не более 4 ГБ, например, `orca-mini-3b-gguf2-q4_0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a79664-8642-4b8a-9b8e-63736bc8f968",
   "metadata": {},
   "source": [
    "| Model name | Model size | Model bin URL |\n",
    "| --- | --- | --- |\n",
    "| ggml-gpt4all-l13b-snoozy| 7.6 GB | http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin |\n",
    "| ggml-gpt4all-j-v1.2-jazzy | 3.8 GB | https://gpt4all.io/models/ggml-gpt4all-j-v1.2-jazzy.bin |\n",
    "| ggml-gpt4all-j-v1.3-groovy | 3.8 GB | https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin |\n",
    "| mistral-7b-openorca.Q4_0 | 3.8 GB | https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf |\n",
    "| mistral-7b-instruct-v0.1.Q4_0 | 3.8 GB | https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf |\n",
    "| gpt4all-falcon-q4_0 | 3.9 GB | https://gpt4all.io/models/gguf/gpt4all-falcon-q4_0.gguf |\n",
    "| wizardlm-13b-v1.2.Q4_0 | 6.9 GB | https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf |\n",
    "| nous-hermes-llama2-13b.Q4_0 | 6.9 GB | https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf |\n",
    "| gpt4all-13b-snoozy-q4_0 | 6.9 GB | https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf |\n",
    "| mpt-7b-chat-merges-q4_0 | 3.5 GB | https://gpt4all.io/models/gguf/mpt-7b-chat-merges-q4_0.gguf |\n",
    "| orca-mini-3b-gguf2-q4_0 | 1.8 GB | https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf |\n",
    "| starcoder-q4_0 | 8.4 GB | https://gpt4all.io/models/gguf/starcoder-q4_0.gguf |\n",
    "| rift-coder-v0-7b-q4_0 | 3.6 GB | https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf |\n",
    "| all-MiniLM-L6-v2-f16 | 44 MB | https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf |\n",
    "| em_german_mistral_v01.Q4_0 | 3.8 GB | https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf |\n",
    "\n",
    "Обратите внимание, что каждая модель поставляется со своей собственной лицензией, и пользователи сами несут ответственность за проверку соответствия их использования лицензии. Подробную информацию о лицензировании вы можете найти на официальном [сайте GPT4All](https://gpt4all.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162a103-c8e4-477a-baed-3e143157c170",
   "metadata": {},
   "source": [
    "## 2. Установка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3375019-d70d-4ace-9c3b-73012868b99d",
   "metadata": {},
   "source": [
    "Когда вы выбрали модель, просто укажите ссылку на нее в переменной `MODEL_URL` в ячейке ниже и запустите ячейку как обычную ячейку в Jupyter. Сейчас в коде в качестве примера используется модель `orca-mini-3b-gguf2-q4_0` и для ее загрузки и переменная `MODEL_URL` принимает значение `https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf`, которое можно найтив колонке `Model bin URL` в таблице выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0af15-54f8-4565-a65a-ba2d096bb735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_URL=\"https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf\"\n",
    "MODEL_PATH=\"/home/jovyan/.cache/gpt4all\"\n",
    "\n",
    "pip install gpt4all==2.1.0\n",
    "rm -rf ${MODEL_PATH}\n",
    "mkdir ${MODEL_PATH}\n",
    "wget -P ${MODEL_PATH} ${MODEL_URL} -q --show-progress --progress=bar:force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c3d0d-3d8a-4926-a492-d85396cce704",
   "metadata": {},
   "source": [
    "Дождитесь выполнения ячейки выше и можно приступать к настройке интерфейса для общения с моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8145a-83c6-4a94-8b30-91bd8e19ffda",
   "metadata": {},
   "source": [
    "## 3. Настройка интерфейса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bc928-f054-4990-b882-18803b4b823d",
   "metadata": {},
   "source": [
    "Для начала необходимо зайти в интерфейс чат-бота через соответствующую иконку, находящуюся слева внизу вертикального бара Jupyter-а, и нажать кнопку перехода в настройки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87220b85-b219-42f1-8a7f-99a82fba832f",
   "metadata": {},
   "source": [
    "![First page](images/aibot_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37055d1-fbab-45f8-bbb6-180e6258a368",
   "metadata": {},
   "source": [
    "В настройках выбрать из выпадающего списка нужную ранее скачанную модель (в нашем примере мы скачали `orca-mini-3b-gguf2-q4_0`) и сохранить эти изменения. Потом нужно выйти в основное меню чат-бота."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777d826-8f34-49b2-a85f-affe17286ac5",
   "metadata": {},
   "source": [
    "![Select model](images/aibot_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908f031-5afd-406b-bd7d-4df9a538c658",
   "metadata": {},
   "source": [
    "Вы увидите интерфейс для общения с LLM моделью. Задавате свои вопросы в диалоговом окне внизу раздела. В ряде случаев ответ может потребовать от модели некотрого времени, так как она развенута локально и не обладает высоким быстродействием."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59823d66-1754-4181-b8cc-e0fc962b486e",
   "metadata": {},
   "source": [
    "![Chat page](images/aibot_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ef352-0593-43fd-ba0d-47bf6450a32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
