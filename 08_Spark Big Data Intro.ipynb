{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b14e1a8",
   "metadata": {},
   "source": [
    "# Spark: Introduction to Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34475a45",
   "metadata": {},
   "source": [
    "[Apache Sparkâ„¢](https://spark.apache.org/) is a unified analytics engine for large-scale data processing. JupyterHub installation offers you a Spark kernel with [PySpark API](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac1f18",
   "metadata": {},
   "source": [
    "__NOTE:__ You should start your server with a `Spark environment` to get advances of `Spark`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39982aba",
   "metadata": {},
   "source": [
    "![Jupyter dashboard showing files tab](images/jupyterlab_spark_env.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ea25e",
   "metadata": {},
   "source": [
    "## Import libraries and get access to Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e715c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e92fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '4g')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49874fa",
   "metadata": {},
   "source": [
    "## Data load from local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9591f",
   "metadata": {},
   "source": [
    "Let's read local stored csv file to `Spark dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\n",
    "    './data/telecom_churn.csv', \n",
    "    sep=',', \n",
    "    header=True\n",
    ")\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c736d8",
   "metadata": {},
   "source": [
    "Spark does not store dataframe in memory and processes it only if some method is called. Below we select first 5 rows of the `Spark dataframe` and convert them to `Pandas dataframe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0978cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f513a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rows in spark dataframe:', sdf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cfca6",
   "metadata": {},
   "source": [
    "## Data process examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59946f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, desc, rank, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).orderBy('Total day minutes').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('Churn').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupby('Voice mail plan').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupby('State').count().sort(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(\n",
    "    'State',\n",
    "    'Voice mail plan',\n",
    "    'Number vmail messages'\n",
    ").filter(\n",
    "    col('State') == 'WY'\n",
    ").limit(\n",
    "    10\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeef6ec",
   "metadata": {},
   "source": [
    "## Apply arbitary function to Spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922f7ef",
   "metadata": {},
   "source": [
    "Apply function to one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f15856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(text):\n",
    "    if text == 'False':\n",
    "        return 0\n",
    "    elif text == 'True':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_one_hot = udf(lambda x: one_hot(x), IntegerType())\n",
    "sdf = sdf.withColumn('Churn_OH', udf_one_hot('Churn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('Churn_OH').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb5b7a",
   "metadata": {},
   "source": [
    "Use two columns as an input to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_cols(x, y):\n",
    "    return int(x) + int(y)\n",
    "\n",
    "sum_cols = udf(sum_of_cols, IntegerType())\n",
    "sdf = sdf.withColumn('Day and Night', sum_cols('Total day calls', 'Total night calls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc817ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('Day and Night').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26753b-f7b9-4951-b687-31c2f005a512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
